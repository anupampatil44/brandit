{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Brand-It-GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA2ipVCQ0Gk9",
        "outputId": "fb727214-f294-4dd5-a0d9-0171f43a9322"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9tTd8E8WXEm"
      },
      "source": [
        "from transformers import GPT2LMHeadModel,GPT2TokenizerFast,GPTNeoModel\n",
        "\n",
        "MODEL_NAME = 'distilgpt2' #'distilgpt2' 'gpt2-medium' 'gpt2-small'\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME,truncation=True,padding=True)\n",
        "# model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Geyt3v9Xtb9",
        "outputId": "7c09d25f-a8c6-4363-8e8a-3dd4227a0047"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MzXOsBHj-W0"
      },
      "source": [
        "import torch\n",
        "# to re-train pre-existing model over more data:\n",
        "model=torch.load('drive/MyDrive/pytorch_hackathon/tagline_generatorv1.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALDsnPsisJmH",
        "outputId": "1a778e12-3cf1-41c1-b5de-f11b99b74c53"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjAPK7wpYjg4",
        "outputId": "8392df01-83b0-46b5-f77e-916bffe481ec"
      },
      "source": [
        "# Declare special tokens for padding and separating the context from the brand statement:\n",
        "additional_tokens = {\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<nameinfo>', '<headline>'],\n",
        "}\n",
        "\n",
        "# Add these special tokens to the vocabulary and resize model's embeddings:\n",
        "tokenizer.add_special_tokens(additional_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Show the full list of special tokens:\n",
        "print(tokenizer.special_tokens_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<nameinfo>', '<headline>']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKBEnqJsiN-Z",
        "outputId": "8c5fcd77-0f4b-4f8b-f404-34a685af444c"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DatasetEncoding(Dataset):\n",
        "  def __init__(self, filename, tokenizer, seq_length=64):\n",
        "\n",
        "    context_token = tokenizer.additional_special_tokens_ids[0]\n",
        "    slogan_token = tokenizer.additional_special_tokens_ids[1]\n",
        "    pad_token = tokenizer.pad_token_id\n",
        "    eos_token = tokenizer.eos_token_id # signifies the end of string\n",
        "\n",
        "    self.examples = []\n",
        "\n",
        "    df=pd.read_csv(filename)\n",
        "    for index,row in df.iterrows():\n",
        "    \n",
        "      # encode the context and slogan segments in the needed format:\n",
        "      context = [context_token] + tokenizer.encode(str(row['name']), max_length=seq_length//2-1)\n",
        "      slogan = [slogan_token] + tokenizer.encode(str(row['headline']), max_length=seq_length//2-2) + [eos_token]\n",
        "      \n",
        "      # Concatenate the two parts together:\n",
        "      tokens = context + slogan + [pad_token] * ( seq_length - len(context) - len(slogan) )\n",
        "\n",
        "      # Annotate each token with its corresponding segment:\n",
        "      segments = [context_token] * len(context) + [slogan_token] * ( seq_length - len(context) )\n",
        "\n",
        "      # Ignore the context, padding, and <slogan> tokens by setting their labels to -100:\n",
        "      labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\n",
        "\n",
        "      # Add the preprocessed examples to the dataset:\n",
        "      self.examples.append((tokens, segments, labels))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return torch.tensor(self.examples[item])\n",
        "\n",
        "\n",
        "# Passing our data for encoding into the appopriate format: \n",
        "slogan_dataset = DatasetEncoding('branding_compiled_dataset.csv', tokenizer)\n",
        "print(next(iter(slogan_dataset)).size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7XSTxJHYKLE"
      },
      "source": [
        "import math, random\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Split data for training and validation splits with a random factor to prevent overfitting cases:\n",
        "\n",
        "indices = list(range(len(slogan_dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "\n",
        "split = math.floor(0.1 * len(slogan_dataset))\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "# declaring data loaders:\n",
        "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9t2RwWNgE4l"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    print('--- Starting epoch #{} ---\\n\\n'.format(i))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    nums = []\n",
        "\n",
        "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
        "      inputs = xb.to(device)\n",
        "\n",
        "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "    \n",
        "      loss = outputs[0]\n",
        "      losses.append(loss.item())\n",
        "      nums.append(len(xb))\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "    # compute average cost over one epoch:\n",
        "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "\n",
        "    # validation:\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      losses = []\n",
        "      nums = []\n",
        "\n",
        "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
        "        inputs = xb.to(device)\n",
        "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
        "        losses.append(outputs[0].item())\n",
        "        nums.append(len(xb))\n",
        "\n",
        "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "    print('\\n--- Epoch #{} finished --- Training cost: {} \\n Validation cost: {}'.format(i, train_cost, val_cost))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPvm_dkUz8I0",
        "outputId": "52991dc9-3114-4481-ddd1-87cd1c6a2287"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Move the model to the GPU:\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tune GPT2 for 3 epochs:\n",
        "optimizer = AdamW(model.parameters())\n",
        "fit(model, optimizer, train_loader, val_loader, epochs=3, device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting epoch #0 ---\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [04:16<00:00,  1.56it/s]\n",
            "Validation: 100%|██████████| 23/23 [00:09<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #0 finished --- Training cost: 1.6407307751933684 \n",
            " Validation cost: 3.155403196307975\n",
            "--- Starting epoch #1 ---\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [04:17<00:00,  1.56it/s]\n",
            "Validation: 100%|██████████| 23/23 [00:09<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #1 finished --- Training cost: 1.107714400273539 \n",
            " Validation cost: 3.526501333881432\n",
            "--- Starting epoch #2 ---\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 400/400 [04:17<00:00,  1.55it/s]\n",
            "Validation: 100%|██████████| 23/23 [00:09<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #2 finished --- Training cost: 0.8498410701154917 \n",
            " Validation cost: 3.8414702113245576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKad2TkQV_VZ"
      },
      "source": [
        "torch.save(model,'drive/MyDrive/pytorch_hackathon/retrained_tagline_generatorv1_compiled.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cS_1D0tZDGG"
      },
      "source": [
        "# Sampling functions with top k and top p from HuggingFace:\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "def top_k_top_p_ordering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or (top-p) filtering.\n",
        "        Args:\n",
        "            logits: batch size x vocabulary size\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "        Snippet taken from: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability greater than the top_p threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "# snippet from HuggingFace, adapted to work for contextual separation:\n",
        "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,):\n",
        "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if segments_tokens != None:\n",
        "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
        "\n",
        "\n",
        "            outputs = model(**inputs) \n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_ordering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # sampling (greedy):\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1EjyJgCaqYN",
        "outputId": "c2517192-e744-4dc5-8ee5-3862203f4bb9"
      },
      "source": [
        "context = \"Tesla, fast luxurious innovation electric cars\"\n",
        "\n",
        "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "input_ids = [context_tkn] + tokenizer.encode(context)\n",
        "\n",
        "segments = [slogan_tkn] * 64\n",
        "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
        "\n",
        "input_ids += [slogan_tkn]\n",
        "\n",
        "# Move the model back to the CPU for inference:\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Generate 20 samples of max length 20\n",
        "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=5)\n",
        "\n",
        "print('\\n\\n--- Generated Marketing statements ---\\n')\n",
        "\n",
        "for g in generated:\n",
        "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
        "  # print(slogan)\n",
        "  slogan = slogan.split('<|endoftext|>')[0].split('<headline>')[1]\n",
        "  print(slogan)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:06<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Generated Marketing statements ---\n",
            "\n",
            "Who could ask for anything more?\n",
            "Knowing how you take care of your car.\n",
            " 7 Star. Carrodisiac.\n",
            "Talk about business.\n",
            "The new way of transportation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zIzYfQQHAyk"
      },
      "source": [
        "inference script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlMeuxxY0gz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d92666f-57a1-466f-b51e-84cf3c5eb4dd"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "import torch\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "def top_k_top_p_ordering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or (top-p) filtering.\n",
        "        Args:\n",
        "            logits: batch size x vocabulary size\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "        Snippet taken from: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability greater than the top_p threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "# snippet from HuggingFace, adapted to work for contextual separation:\n",
        "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,):\n",
        "    context = torch.tensor(context, dtype=torch.long, device='cpu')\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if segments_tokens != None:\n",
        "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
        "\n",
        "\n",
        "            outputs = model(**inputs) \n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_ordering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # sampling (greedy):\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2',truncation=True,padding=True)\n",
        "model=torch.load('drive/MyDrive/pytorch_hackathon/retrained_tagline_generatorv1_compiled.pth')\n",
        "\n",
        "extra_tokens = {\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<nameinfo>', '<headline>']\n",
        "}\n",
        "\n",
        "\n",
        "tokenizer.add_special_tokens(extra_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "context = \"Tesla, fast luxurious electric cars\"\n",
        "\n",
        "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "input_ids = [context_tkn] + tokenizer.encode(context)\n",
        "\n",
        "segments = [slogan_tkn] * 64\n",
        "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
        "\n",
        "input_ids += [slogan_tkn]\n",
        "\n",
        "# Move the model back to the CPU for inference:\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Generate 20 samples of max length 20\n",
        "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=5)\n",
        "\n",
        "for g in generated:\n",
        "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
        "  slogan = slogan.split('<|endoftext|>')[0].split('<headline>')[1]\n",
        "  print(slogan)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:06<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The power of choice.\n",
            "Giving you the transport you need.\n",
            "Only 5 Star is the clinic.\n",
            "Who could ask for anything more?\n",
            "Emotional luxury.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}